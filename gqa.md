# Description
Visual Question Answering (VQA) is a compelling challenge in multimodal machine learning, demanding intricate reasoning about the visual world through natural language. Recognizing that a comprehensive understanding necessitates more than just high-level visual features, this research delves into strategically incorporating scene graphs as a potent additional modality. By explicitly encoding objects and their rich interconnected relationships within a visual scene, scene graphs offer a structured pathway for VQA models to move beyond superficial correlations and engage in deeper, more human-like reasoning. This Multimodal ML class project investigates the multifaceted advantages of leveraging scene graph information to significantly enhance VQA performance, particularly by focusing on the following innovative approaches explored in the paper:

1. Within the architecture of coarse-to-fine reasoning, where the model utilizes different semantic levels of reasoning to better understand inputs, scene graphs can serve as a rich component to the fine-grain module by providing object and inter-object descriptions within the scene. Here, scene graphs serve as a cornerstone of the fine-grained reasoning module, meticulously processing object-level details and their interactions. By iteratively applying instruction vectors derived from the question over the scene graph using a Graph Attention Network (GAT), the model gains a nuanced understanding of the visual relationships, complementing the broader contextual awareness provided by the coarse reasoning module that utilizes pre-trained vision-language models like LXMERT on global object features. The fusion of these semantic levels allows the model to selectively leverage both holistic and granular visual cues to arrive at accurate answers.
2. Within the task of pre-training models with scene graphs, relationships between objects in the graphical representation can be masked out, providing a more concrete training example for the model to better understand relationships in a visual scene. Notably, the task of Scene Graph Masking (SM) is explored, where relationships between objects within the scene graph are intentionally masked, compelling the model to predict these missing links. This targeted pre-training fosters a deeper understanding of the intricate relational dynamics inherent in visual scenes, enabling the model to reason about object interactions during downstream VQA tasks more effectively.
3. Within low resource, or low example, settings, scene graphs can provide an additional resource in the case of prompt tuning, where one input can be expanded to many based on various prompt augmentation methods. For prompt tuning, scene graph-based features are investigated as a method for effective prompt initialization, guiding pre-trained language models towards improved performance even with limited examples. In addition, scene graphs can be generated within datasets that do not inherently support them by using trained scene graph generation models such as [Neural Motifs](https://arxiv.org/pdf/1711.06640).


# [Paper link](/assets/pdf/gqa.pdf)
